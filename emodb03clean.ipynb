{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SincNet EmoDB: 10-Fold Cross-Validation Framework\nThis notebook implements a clean evaluation pipeline for SincNet-based models on the EmoDB dataset.\nIt compares:\n1. **Basic SincNet**: The original architecture from Ravanelli et al. (2018).\n2. **SincNet-LSTM-Attention**: An enhanced architecture for capturing temporal emotion dynamics.\n\nEvaluation Protocol: **Speaker-Independent 10-Fold Cross-Validation**.","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport math\nimport copy\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport soundfile as sf\nimport librosa\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:09:40.896896Z","iopub.execute_input":"2026-01-06T16:09:40.897194Z","iopub.status.idle":"2026-01-06T16:09:45.771581Z","shell.execute_reply.started":"2026-01-06T16:09:40.897167Z","shell.execute_reply":"2026-01-06T16:09:45.770756Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!git clone https://github.com/mathispernin/sincnet_dlts.git\n%cd sincnet_dlts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:09:45.772848Z","iopub.execute_input":"2026-01-06T16:09:45.773224Z","iopub.status.idle":"2026-01-06T16:09:49.551767Z","shell.execute_reply.started":"2026-01-06T16:09:45.773199Z","shell.execute_reply":"2026-01-06T16:09:49.551047Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'sincnet_dlts'...\nremote: Enumerating objects: 544, done.\u001b[K\nremote: Counting objects: 100% (544/544), done.\u001b[K\nremote: Compressing objects: 100% (543/543), done.\u001b[K\nremote: Total 544 (delta 0), reused 541 (delta 0), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (544/544), 40.63 MiB | 18.44 MiB/s, done.\n/kaggle/working/sincnet_dlts\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- CONFIGURATION ---\nDATA_PATH = 'data/'  # Ensure your EmoDB .wav files are in this folder\nNUM_EPOCHS = 50\nBATCH_SIZE = 32\nLEARNING_RATE = 0.001\n\n# EmoDB specific constants\nALL_SPEAKERS = ['03', '08', '09', '10', '11', '12', '13', '14', '15', '16']\nEMOTION_MAP = {\n    'W': 0, 'L': 1, 'E': 2, 'A': 3, 'F': 4, 'T': 5, 'N': 6\n}\nEMOTION_NAMES = ['Anger', 'Boredom', 'Disgust', 'Fear', 'Happiness', 'Sadness', 'Neutral']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:09:49.553080Z","iopub.execute_input":"2026-01-06T16:09:49.553338Z","iopub.status.idle":"2026-01-06T16:09:49.557863Z","shell.execute_reply.started":"2026-01-06T16:09:49.553311Z","shell.execute_reply":"2026-01-06T16:09:49.557283Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"# --- AUGMENTATION FUNCTIONS ---\ndef augment_noise(data):\n    \"\"\"Inject white noise.\"\"\"\n    noise_amp = 0.005 * np.random.uniform() * np.amax(data)\n    data = data + noise_amp * np.random.normal(size=data.shape[0])\n    return data\n\ndef augment_pitch(data, sr=16000):\n    \"\"\"Pitch shift.\"\"\"\n    n_steps = np.random.uniform(-2, 2)\n    return librosa.effects.pitch_shift(y=data, sr=sr, n_steps=n_steps)\n\ndef augment_speed(data):\n    \"\"\"Speed change (fast/slow).\"\"\"\n    speed_factor = np.random.uniform(0.9, 1.1)\n    return librosa.effects.time_stretch(y=data, rate=speed_factor)\n\n# --- FILE PARSING UTILS ---\ndef get_files_for_speakers(speaker_list, data_path):\n    if not os.path.exists(data_path):\n        print(f\"ERROR: Path '{data_path}' does not exist!\")\n        return []\n        \n    all_files = [f for f in os.listdir(data_path) if f.endswith('.wav')]\n    target_files = []\n    # Regex to match EmoDB format (e.g., 03a01Fa.wav)\n    regex = r'(\\d{2})([a-z]\\d{2})([A-Z])([a-z]?)\\.wav'\n    \n    for f in all_files:\n        m = re.match(regex, f)\n        if m:\n            spk = m.group(1)\n            if spk in speaker_list:\n                target_files.append(f)\n    return target_files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:09:49.559448Z","iopub.execute_input":"2026-01-06T16:09:49.559699Z","iopub.status.idle":"2026-01-06T16:09:49.572790Z","shell.execute_reply.started":"2026-01-06T16:09:49.559679Z","shell.execute_reply":"2026-01-06T16:09:49.572039Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class EmoDBDataset(Dataset):\n    def __init__(self, data_path, file_list, max_len=48000, augment=False):\n        self.data_path = data_path\n        self.files = file_list\n        self.max_len = max_len\n        self.augment = augment\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        file_name = self.files[idx]\n        file_path = os.path.join(self.data_path, file_name)\n        \n        # Load audio\n        wav, sr = sf.read(file_path)\n        wav = wav.astype(np.float32) \n        \n        # --- Augmentation Pipeline ---\n        if self.augment:\n            aug_choice = np.random.randint(0, 4)\n            try:\n                if aug_choice == 0:\n                    wav = augment_noise(wav)\n                elif aug_choice == 1:\n                    wav = augment_pitch(wav, sr)\n                elif aug_choice == 2:\n                    wav = augment_speed(wav)\n                # Choice 3 is \"no augmentation\"\n            except Exception:\n                pass\n\n        # --- Padding / Truncating ---\n        if len(wav) < self.max_len:\n            pad = self.max_len - len(wav)\n            wav = np.pad(wav, (0, pad), 'constant')\n        else:\n            diff = len(wav) - self.max_len\n            if self.augment and diff > 0:\n                # Random crop for training\n                start = np.random.randint(0, diff)\n                wav = wav[start : start + self.max_len]\n            else:\n                # Center crop/Fixed crop for validation/testing\n                wav = wav[:self.max_len]\n\n        # --- Label Extraction ---\n        m = re.match(r'(\\d{2})([a-z]\\d{2})([A-Z])([a-z]?)\\.wav', file_name)\n        if m:\n            emotion_code = m.group(3)\n            label = EMOTION_MAP[emotion_code]\n        else:\n            label = 0 # Should not happen with correct filtering\n\n        return torch.FloatTensor(wav).unsqueeze(0), torch.tensor(label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:09:49.573908Z","iopub.execute_input":"2026-01-06T16:09:49.574246Z","iopub.status.idle":"2026-01-06T16:09:49.591638Z","shell.execute_reply.started":"2026-01-06T16:09:49.574223Z","shell.execute_reply":"2026-01-06T16:09:49.590924Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Models Architectures","metadata":{}},{"cell_type":"code","source":"# --- 1. SincConv Layer ---\nclass SincConv_fast(nn.Module):\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10 ** (mel / 2595) - 1)\n\n    def __init__(self, out_channels, kernel_size, sample_rate=16000, in_channels=1,\n                 stride=1, padding=0, dilation=1, min_low_hz=50, min_band_hz=50):\n        super(SincConv_fast,self).__init__()\n\n        if in_channels != 1:\n            raise ValueError(\"SincConv only supports one input channel.\")\n\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        \n        # Force odd kernel size\n        if kernel_size % 2 == 0:\n            self.kernel_size = self.kernel_size + 1\n\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.sample_rate = sample_rate\n        self.min_low_hz = min_low_hz\n        self.min_band_hz = min_band_hz\n\n        # Initialize filterbanks (Mel-scale)\n        low_hz = 30\n        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n\n        mel = np.linspace(self.to_mel(low_hz), self.to_mel(high_hz), self.out_channels + 1)\n        hz = self.to_hz(mel)\n\n        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n\n        # Hamming window\n        self.window_ = torch.hamming_window(self.kernel_size, periodic=False)\n        n = (self.kernel_size - 1) / 2.0\n        self.n_ = 2 * math.pi * torch.arange(-n, 0).view(1, -1) / self.sample_rate\n\n    def forward(self, waveforms):\n        self.n_ = self.n_.to(waveforms.device)\n        self.window_ = self.window_.to(waveforms.device)\n\n        low = self.min_low_hz + torch.abs(self.low_hz_)\n        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_), self.min_low_hz, self.sample_rate/2)\n        band = (high - low)[:, 0]\n\n        f_times_t_low = torch.matmul(low, self.n_)\n        f_times_t_high = torch.matmul(high, self.n_)\n\n        band_pass_left = ((torch.sin(f_times_t_high) - torch.sin(f_times_t_low)) / (self.n_ / 2))\n        band_pass_center = 2 * band.view(-1, 1)\n        band_pass_right = torch.flip(band_pass_left, dims=[1])\n\n        band_pass = torch.cat([band_pass_left, band_pass_center, band_pass_right], dim=1)\n        band_pass = band_pass / (2 * band[:, None])\n\n        self.filters = (band_pass * self.window_)\n\n        return F.conv1d(waveforms, self.filters.view(self.out_channels, 1, self.kernel_size),\n                        stride=self.stride, padding=self.padding, dilation=self.dilation,\n                        bias=None, groups=1)\n\n# --- 2. Attention Layer ---\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super(Attention, self).__init__()\n        self.attention = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        scores = self.attention(x) \n        weights = F.softmax(scores, dim=1)\n        context_vector = torch.sum(x * weights, dim=1)\n        return context_vector","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:09:49.592564Z","iopub.execute_input":"2026-01-06T16:09:49.592916Z","iopub.status.idle":"2026-01-06T16:09:49.612161Z","shell.execute_reply.started":"2026-01-06T16:09:49.592886Z","shell.execute_reply":"2026-01-06T16:09:49.611506Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class SincNet_Basic(nn.Module):\n    \"\"\"\n    Standard SincNet architecture: SincConv -> MaxPool -> CNNs -> MLP\n    \"\"\"\n    def __init__(self, num_classes=7):\n        super(SincNet_Basic, self).__init__()\n        \n        self.sinc_conv = SincConv_fast(out_channels=80, kernel_size=251, sample_rate=16000)\n        self.pool = nn.MaxPool1d(3)\n        self.bn0 = nn.BatchNorm1d(80)\n        \n        self.conv2 = nn.Conv1d(80, 60, kernel_size=5)\n        self.bn2 = nn.BatchNorm1d(60)\n        \n        self.conv3 = nn.Conv1d(60, 60, kernel_size=5)\n        self.bn3 = nn.BatchNorm1d(60)\n        \n        self.fc1 = nn.Linear(60, 256)\n        self.bn_fc1 = nn.BatchNorm1d(256)\n        self.fc2 = nn.Linear(256, num_classes)\n        \n    def forward(self, x):\n        # x: (Batch, 1, Seq)\n        x = self.sinc_conv(x)\n        x = self.pool(F.leaky_relu(self.bn0(x)))\n        \n        x = self.pool(F.leaky_relu(self.bn2(self.conv2(x))))\n        x = self.pool(F.leaky_relu(self.bn3(self.conv3(x))))\n        \n        x = x.mean(dim=2) # Global Averaging\n        \n        x = F.leaky_relu(self.bn_fc1(self.fc1(x)))\n        x = self.fc2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:09:49.612882Z","iopub.execute_input":"2026-01-06T16:09:49.613191Z","iopub.status.idle":"2026-01-06T16:09:49.628541Z","shell.execute_reply.started":"2026-01-06T16:09:49.613169Z","shell.execute_reply":"2026-01-06T16:09:49.627952Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# --- 3. Full Model ---\nclass SincNetLSTM_Attention(nn.Module):\n    def __init__(self, num_classes=7):\n        super(SincNetLSTM_Attention, self).__init__()\n        \n        # Frontend: SincNet\n        self.sinc_conv = SincConv_fast(out_channels=80, kernel_size=251, sample_rate=16000)\n        self.bn0 = nn.BatchNorm1d(80)\n        self.pool0 = nn.MaxPool1d(3)\n\n        # Standard CNN\n        self.conv1 = nn.Conv1d(80, 64, kernel_size=5, padding=2)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.pool1 = nn.MaxPool1d(2)\n        self.drop1 = nn.Dropout(0.3)\n\n        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.pool2 = nn.MaxPool1d(2)\n        self.drop2 = nn.Dropout(0.3)\n\n        # Backend: LSTM\n        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=1, batch_first=True, bidirectional=True)\n        self.drop_lstm = nn.Dropout(0.3)\n\n        # Attention\n        self.attention = Attention(hidden_dim=256)\n\n        # Classifier\n        self.fc1 = nn.Linear(256, 64)\n        self.bn_fc1 = nn.BatchNorm1d(64)\n        self.drop_fc = nn.Dropout(0.4)\n        self.fc2 = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = self.sinc_conv(x)\n        x = F.relu(self.bn0(x))\n        x = self.pool0(x) \n        \n        x = self.conv1(x)\n        x = F.relu(self.bn1(x))\n        x = self.pool1(x)\n        x = self.drop1(x)\n        \n        x = self.conv2(x)\n        x = F.relu(self.bn2(x))\n        x = self.pool2(x)\n        x = self.drop2(x)\n        \n        x = x.permute(0, 2, 1) # (Batch, Time, Feats)\n        \n        self.lstm.flatten_parameters()\n        x, _ = self.lstm(x)\n        x = self.drop_lstm(x)\n        \n        x = self.attention(x)\n        \n        x = self.fc1(x)\n        x = F.relu(self.bn_fc1(x))\n        x = self.drop_fc(x)\n        x = self.fc2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:09:49.629353Z","iopub.execute_input":"2026-01-06T16:09:49.629606Z","iopub.status.idle":"2026-01-06T16:09:49.649486Z","shell.execute_reply.started":"2026-01-06T16:09:49.629584Z","shell.execute_reply":"2026-01-06T16:09:49.648893Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"def train_one_fold(model_class, train_loader, val_loader, device, num_epochs=40):\n    # Instantiate fresh model\n    model = model_class(num_classes=7).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4)\n    criterion = nn.CrossEntropyLoss()\n    \n    best_acc = 0.0\n    best_weights = copy.deepcopy(model.state_dict())\n    \n    for epoch in range(num_epochs):\n        model.train()\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            loss = criterion(model(inputs), labels)\n            loss.backward()\n            optimizer.step()\n            \n        model.eval()\n        correct, total, val_loss = 0, 0, 0.0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                val_loss += criterion(outputs, labels).item() * inputs.size(0)\n                _, pred = torch.max(outputs, 1)\n                correct += (pred == labels).sum().item()\n                total += labels.size(0)\n        \n        if total > 0:\n            acc = correct / total\n            scheduler.step(val_loss / total)\n            if acc > best_acc:\n                best_acc = acc\n                best_weights = copy.deepcopy(model.state_dict())\n                \n    return best_weights, best_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:09:49.650301Z","iopub.execute_input":"2026-01-06T16:09:49.650569Z","iopub.status.idle":"2026-01-06T16:09:49.664906Z","shell.execute_reply.started":"2026-01-06T16:09:49.650538Z","shell.execute_reply":"2026-01-06T16:09:49.664342Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def run_cross_validation(model_class, model_name=\"Model\", epochs=40):\n    print(f\"\\n[START] Cross-Validation for: {model_name}\")\n    print(\"=\"*50)\n    \n    fold_accuracies = []\n    \n    for i, test_speaker in enumerate(ALL_SPEAKERS):\n        # Split Strategy\n        val_idx = (i + 1) % len(ALL_SPEAKERS)\n        val_speaker = ALL_SPEAKERS[val_idx]\n        train_speakers = [s for s in ALL_SPEAKERS if s != test_speaker and s != val_speaker]\n        \n        # Get Files\n        train_files = get_files_for_speakers(train_speakers, DATA_PATH)\n        val_files   = get_files_for_speakers([val_speaker], DATA_PATH)\n        test_files  = get_files_for_speakers([test_speaker], DATA_PATH)\n        \n        if not train_files or not test_files: continue\n            \n        # Create Loaders\n        train_dl = DataLoader(EmoDBDataset(DATA_PATH, train_files, augment=True), \n                              batch_size=32, shuffle=True, drop_last=True)\n        val_dl   = DataLoader(EmoDBDataset(DATA_PATH, val_files), batch_size=32)\n        test_dl  = DataLoader(EmoDBDataset(DATA_PATH, test_files), batch_size=32)\n        \n        # Train\n        print(f\"Fold {i+1}/{len(ALL_SPEAKERS)} (Test: {test_speaker}) ... \", end=\"\")\n        best_weights, val_acc = train_one_fold(model_class, train_dl, val_dl, device, epochs)\n        \n        # Test\n        model = model_class(num_classes=7).to(device)\n        model.load_state_dict(best_weights)\n        model.eval()\n        \n        preds, truths = [], []\n        with torch.no_grad():\n            for inputs, labels in test_dl:\n                inputs = inputs.to(device)\n                preds.extend(torch.max(model(inputs), 1)[1].cpu().numpy())\n                truths.extend(labels.numpy())\n        \n        acc = accuracy_score(truths, preds)\n        fold_accuracies.append(acc)\n        print(f\"Test Acc: {acc*100:.2f}% (Val: {val_acc*100:.2f}%)\")\n        \n    avg = np.mean(fold_accuracies)\n    std = np.std(fold_accuracies)\n    print(\"=\"*50)\n    print(f\"[{model_name}] Final Accuracy: {avg*100:.2f}% (+/- {std*100:.2f}%)\")\n    print(\"=\"*50)\n    return fold_accuracies","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:14:06.168432Z","iopub.execute_input":"2026-01-06T16:14:06.169189Z","iopub.status.idle":"2026-01-06T16:14:06.177652Z","shell.execute_reply.started":"2026-01-06T16:14:06.169157Z","shell.execute_reply":"2026-01-06T16:14:06.176934Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# 1. Train Basic SincNet\nacc_basic = run_cross_validation(SincNet_Basic, \"Basic SincNet\", epochs=40)\n\n# 2. Train AdvaSincNet (LSTM+Attention)\nacc_advanced = run_cross_validation(SincNetLSTM_Attention, \"SincNet+LSTM+Attn\", epochs=40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T16:14:07.800482Z","iopub.execute_input":"2026-01-06T16:14:07.800819Z"}},"outputs":[{"name":"stdout","text":"\n[START] Cross-Validation for: Basic SincNet\n==================================================\nFold 1/10 (Test: 03) ... Test Acc: 46.94% (Val: 77.59%)\nFold 2/10 (Test: 08) ... Test Acc: 62.07% (Val: 55.81%)\nFold 3/10 (Test: 09) ... Test Acc: 27.91% (Val: 84.21%)\nFold 4/10 (Test: 10) ... Test Acc: 73.68% (Val: 58.18%)\nFold 5/10 (Test: 11) ... Test Acc: 54.55% (Val: 62.86%)\nFold 6/10 (Test: 12) ... Test Acc: 60.00% (Val: 67.21%)\nFold 7/10 (Test: 13) ... Test Acc: 50.82% (Val: 63.77%)\nFold 8/10 (Test: 14) ... Test Acc: 47.83% (Val: 58.93%)\nFold 9/10 (Test: 15) ... Test Acc: 51.79% (Val: 60.56%)\nFold 10/10 (Test: 16) ... Test Acc: 52.11% (Val: 59.18%)\n==================================================\n[Basic SincNet] Final Accuracy: 52.77% (+/- 11.23%)\n==================================================\n\n[START] Cross-Validation for: SincNet+LSTM+Attn\n==================================================\nFold 1/10 (Test: 03) ... Test Acc: 46.94% (Val: 67.24%)\nFold 2/10 (Test: 08) ... Test Acc: 58.62% (Val: 48.84%)\nFold 3/10 (Test: 09) ... Test Acc: 32.56% (Val: 84.21%)\nFold 4/10 (Test: 10) ... ","output_type":"stream"}],"execution_count":null}]}